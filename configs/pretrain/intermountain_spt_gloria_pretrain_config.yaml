experiment_name: 'intermountain_spt_gloria_pretrain_lr_0.00005'
phase: 'pretrain'

lightning:
    trainer:
       gpus: '0'
       max_epochs: 50
       distributed_backend: 'dp'
       gradient_clip_val: 0.25
       lr: 0.00005
       precision: 16
    checkpoint_callback:
        monitor: 'val_loss'
        dirpath: './data/ckpt'
        save_last: true 
        mode: min
        save_top_k: 10
    early_stopping_callback:
        monitor: 'val_loss'
        min_delta: 0.00
        patience: 10
        verbose: False
        mode: 'min'
    logger:
        logger_type: 'WandbLogger'
        save_dir: './data/'
        project: 'gloria_pretrain'

model: 
    gloria: 
        local_loss_weight: 1.0
        global_loss_weight: 1.0
        temp1: 4.0
        temp2: 5.0
        temp3: 10.0
    vision:
        model_name: 'densenet_121'
        freeze_cnn: false
        pretrained: true
    text:  
        bert_type: "emilyalsentzer/Bio_ClinicalBERT"
        model_max_length: 128
        last_n_layers: 4
        aggregate_method: 'sum'
        norm: false
        embedding_dim: 768
        freeze_bert: false
        agg_tokens: true
data: 
    dataset: intermountain
    text: 
      dword_num: 128
      captions_per_image: 5
      full_report: false
    image:
        imsize: 256

transforms: 
    norm: 'half'
    random_crop:
        crop_size: 224

train: 
    update_interval: 1000
    batch_size: 48
    num_workers: 18
    nvis: 8
    rand_vis: false 
    optimizer: 
        name: 'Adam'
        weight_decay: 1e-6
    scheduler: 
        name: 'plateau'
        monitor: 'val_loss'
        inerval: 'epoch'
        frequency: 1
